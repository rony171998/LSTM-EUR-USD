#!/usr/bin/env python3
"""
evaluate_optuna_models_rolling_advanced.py - Rolling Forecast Avanzado
Implementa re-entrenamiento incremental que hizo exitoso a ARIMA

üéØ CONFIGURACI√ìN PARA 72.41% DA GRU:
- Seed base: 123
- √âpocas por modelo: 15 (en lugar de 10)
- Feature set: basic (returns, rsi, sma20, dxy_returns)
- Scaler: RobustScaler
- Seed espec√≠fico por predicci√≥n: 123 + i * 7
"""

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import json
import time
from datetime import timedelta
from sklearn.preprocessing import RobustScaler
from sklearn.metrics import mean_squared_error, r2_score
import warnings
warnings.filterwarnings('ignore')

from config import DEFAULT_PARAMS
from modelos import (
    TLS_LSTMModel,
    GRU_Model,
    HybridLSTMAttentionModel,
    BidirectionalDeepLSTMModel,
)

# üéØ CONFIGURACI√ìN EXACTA PARA 72.41% DA
# Aplicar seed 123 al inicio del script completo
torch.manual_seed(123)
np.random.seed(123)
if torch.cuda.is_available():
    torch.cuda.manual_seed(123)
    torch.cuda.manual_seed_all(123)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

device = torch.device("cuda")

def load_multi_asset_data():
    """Cargar EUR/USD + DXY EXACTAMENTE como en el script exitoso de 72.41% DA"""
    print("üìä Cargando EUR/USD + DXY...")
    
    current_dir = Path.cwd()
    if current_dir.name == "model":
        data_prefix = "../data/"
    else:
        data_prefix = "data/"
        
    # EUR/USD con conversor EXACTO
    eur_file = f"{data_prefix}{DEFAULT_PARAMS.FILEPATH}"
    eur_df = pd.read_csv(
        eur_file,
        index_col="Fecha",
        parse_dates=True,
        dayfirst=True,
        decimal=",",
        thousands=".",
        converters={
            "√öltimo": lambda x: float(str(x).replace(".", "").replace(",", ".")) if x else np.nan
        }
    )
    eur_df = eur_df.sort_index(ascending=True)
    eur_prices = eur_df["√öltimo"].dropna()
    
    # DXY con conversor EXACTO
    dxy_file = f"{data_prefix}DXY_2010-2024.csv"
    dxy_df = pd.read_csv(
        dxy_file,
        index_col="Fecha", 
        parse_dates=True,
        dayfirst=True,
        decimal=",",
        thousands=".",
        converters={
            "√öltimo": lambda x: float(str(x).replace(".", "").replace(",", ".")) if x else np.nan
        }
    )
    dxy_df = dxy_df.sort_index(ascending=True)
    dxy_prices = dxy_df["√öltimo"].dropna()
    
    print(f"   ‚úÖ EUR/USD: {len(eur_prices)} registros")
    print(f"   ‚úÖ DXY: {len(dxy_prices)} registros")
    
    return eur_prices, dxy_prices

def create_proven_features(eur_prices, dxy_prices):
    """Crear caracter√≠sticas EXACTAMENTE como en el script exitoso de 72.41% DA"""
    print("üîß Creando caracter√≠sticas probadas...")
    
    # Alinear fechas comunes
    common_dates = eur_prices.index.intersection(dxy_prices.index)
    eur_aligned = eur_prices.loc[common_dates]
    dxy_aligned = dxy_prices.loc[common_dates]
    
    # 1. EUR/USD returns (CLAVE)
    eur_returns = eur_aligned.pct_change()
    
    # 2. DXY returns (CLAVE)
    dxy_returns = dxy_aligned.pct_change()
    
    # 3. RSI
    def calculate_rsi(prices, window=14):
        delta = prices.diff()
        gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()
        rs = gain / loss
        return 100 - (100 / (1 + rs))
    
    eur_rsi = calculate_rsi(eur_aligned)
    
    # 4. SMA20
    eur_sma20 = eur_aligned.rolling(window=20).mean()
    
    # Crear DataFrame final EXACTO
    features_dict = {
        'price': eur_aligned,
        'returns': eur_returns,
        'rsi': eur_rsi,
        'sma20': eur_sma20,
        'dxy_returns': dxy_returns
    }
    
    features_df = pd.DataFrame(features_dict)
    features_df = features_df.dropna()
    
    print(f"‚úÖ Caracter√≠sticas: {features_df.shape}")
    print(f"   Features: {list(features_df.columns)}")
    
    return features_df

def prepare_full_data():
    """Preparar datos completos EXACTAMENTE como en el script exitoso"""
    eur_prices, dxy_prices = load_multi_asset_data()
    features_df = create_proven_features(eur_prices, dxy_prices)
    
    target_data = features_df['price']
    feature_columns = [col for col in features_df.columns if col != 'price']
    features_data = features_df[feature_columns]
    
    # Split temporal (80/20)
    train_size = int(len(features_data) * 0.8)
    
    return {
        'features_data': features_data,
        'target_data': target_data,
        'train_size': train_size,
        'dates': features_data.index
    }

def incremental_rolling_forecast_lstm(model, full_data, seq_length, optuna_params, max_predictions=50):
    """
    üöÄ ROLLING FORECAST AVANZADO CON RE-ENTRENAMIENTO
    Clave del √©xito de ARIMA: re-entrenar con datos nuevos
    """
    print(f"üîÑ Rolling Forecast Avanzado (m√°ximo {max_predictions} predicciones)...")
    print("üéØ Incluye: Re-entrenamiento incremental + Actualizaci√≥n caracter√≠sticas")
    
    features_data = full_data['features_data']
    target_data = full_data['target_data']
    train_size = full_data['train_size']
    
    # Inicializar con datos de entrenamiento
    current_train_end = train_size
    predictions = []
    real_values = []
    
    # Limitar predicciones
    max_possible = len(target_data) - train_size - seq_length
    num_predictions = min(max_predictions, max_possible)
    
    print(f"   üìä Entrenamiento inicial hasta: {current_train_end}")
    print(f"   üéØ Predicciones a realizar: {num_predictions}")
    
    for i in range(num_predictions):
        print(f"   üîÑ Predicci√≥n {i+1}/{num_predictions}")
        
        # 1. PREPARAR DATOS HASTA EL PUNTO ACTUAL
        X_train_raw = features_data.iloc[:current_train_end].values
        y_train_raw = target_data.iloc[:current_train_end].values
        
        # Escalado EXACTO como en script exitoso (SIN par√°metros espec√≠ficos)
        scaler = RobustScaler()  # CAMBIO CLAVE: sin quantile_range
        X_train_scaled = scaler.fit_transform(X_train_raw)
        
        target_scaler = RobustScaler()  # CAMBIO CLAVE: sin quantile_range
        y_train_scaled = target_scaler.fit_transform(y_train_raw.reshape(-1, 1)).flatten()
        
        # Crear secuencias EXACTAMENTE como en script exitoso
        def create_sequences(X, y, seq_len):
            X_seq, y_seq = [], []
            for j in range(seq_len, len(X)):
                X_seq.append(X[j-seq_len:j])
                y_seq.append(y[j])
            return np.array(X_seq), np.array(y_seq)
        
        X_train_seq, y_train_seq = create_sequences(X_train_scaled, y_train_scaled, seq_length)
        
        if len(X_train_seq) < 10:  # Verificar datos m√≠nimos
            print(f"   ‚ö†Ô∏è Datos insuficientes para predicci√≥n {i+1}")
            break
        
        # 2. RE-ENTRENAR MODELO CON DATOS ACTUALIZADOS
        input_size = X_train_seq.shape[2]
        
        # üéØ Seed espec√≠fico para configuraci√≥n ganadora 72.41% DA
        torch.manual_seed(123 + i * 7)
        np.random.seed(123 + i * 7)
        if torch.cuda.is_available():
            torch.cuda.manual_seed(123 + i * 7)
        
        # Crear modelo fresco (esto es clave para ARIMA-like behavior)
        if model.__class__.__name__ == "TLS_LSTMModel":
            fresh_model = TLS_LSTMModel(
                input_size=input_size,
                hidden_size=optuna_params['hidden_size'],
                output_size=1,
                dropout_prob=optuna_params['dropout_prob']
            ).to(device)
        elif model.__class__.__name__ == "GRU_Model":
            fresh_model = GRU_Model(
                input_size=input_size,
                hidden_size=optuna_params['hidden_size'],
                output_size=1,
                dropout_prob=optuna_params['dropout_prob'],
                num_layers=2
            ).to(device)
        elif model.__class__.__name__ == "HybridLSTMAttentionModel":
            fresh_model = HybridLSTMAttentionModel(
                input_size=input_size,
                hidden_size=optuna_params['hidden_size'],
                output_size=1,
                dropout_prob=optuna_params['dropout_prob']
            ).to(device)
        elif model.__class__.__name__ == "BidirectionalDeepLSTMModel":
            fresh_model = BidirectionalDeepLSTMModel(
                input_size=input_size,
                hidden_size=optuna_params['hidden_size'],
                output_size=1,
                dropout_prob=optuna_params['dropout_prob']
            ).to(device)
        else:
            print(f"   ‚ö†Ô∏è Modelo no soportado: {model.__class__.__name__}")
            break
        
        # 3. ENTRENAMIENTO R√ÅPIDO EXACTO (como en script exitoso)
        X_train_tensor = torch.FloatTensor(X_train_seq).to(device)
        y_train_tensor = torch.FloatTensor(y_train_seq).to(device)  # USAR y_train_seq como script exitoso
        
        optimizer = optim.Adam(fresh_model.parameters(), lr=optuna_params['learning_rate'])
        criterion = nn.MSELoss()
        
        fresh_model.train()
        
        # üéØ 15 √©pocas para lograr 72.41% DA (configuraci√≥n ganadora)
        for epoch in range(15):
            optimizer.zero_grad()
            outputs = fresh_model(X_train_tensor).squeeze()
            loss = criterion(outputs, y_train_tensor)
            loss.backward()
            optimizer.step()
        
        # 4. HACER PREDICCI√ìN
        # Preparar √∫ltima secuencia para predicci√≥n
        if current_train_end + seq_length <= len(features_data):
            X_pred_raw = features_data.iloc[current_train_end-seq_length:current_train_end].values
            X_pred_scaled = scaler.transform(X_pred_raw)
            X_pred_tensor = torch.FloatTensor(X_pred_scaled).unsqueeze(0).to(device)
            
            fresh_model.eval()
            with torch.no_grad():
                pred_scaled = fresh_model(X_pred_tensor).squeeze()
                pred_value = target_scaler.inverse_transform(
                    pred_scaled.cpu().numpy().reshape(-1, 1)
                ).flatten()[0]
            
            # üîß VALIDAR PREDICCI√ìN (detectar NaN/Inf)
            if np.isnan(pred_value) or np.isinf(pred_value):
                print(f"      ‚ö†Ô∏è Predicci√≥n inv√°lida detectada: {pred_value}")
                # Usar √∫ltimo valor v√°lido como fallback
                if len(predictions) > 0:
                    pred_value = predictions[-1]  # √öltimo valor v√°lido
                    print(f"      üîß Usando fallback: {pred_value:.6f}")
                else:
                    pred_value = target_data.iloc[current_train_end-1]  # Valor anterior
                    print(f"      üîß Usando valor anterior: {pred_value:.6f}")
            
            predictions.append(pred_value)
            
            # Valor real correspondiente
            real_value = target_data.iloc[current_train_end]
            real_values.append(real_value)
            
            print(f"      üìà Pred: {pred_value:.6f}, Real: {real_value:.6f}")
        
        # 5. AVANZAR VENTANA (incluir nuevo valor real)
        current_train_end += 1
        
        # Evitar que el bucle sea muy largo
        if current_train_end >= len(target_data) - 1:
            break
    
    predictions = np.array(predictions)
    real_values = np.array(real_values)
    
    print(f"   ‚úÖ Rolling Avanzado completado: {len(predictions)} predicciones")
    
    return predictions, real_values

def evaluate_model_advanced_rolling(model_path):
    """Evaluar modelo con Rolling Forecast Avanzado"""
    try:
        # Cargar checkpoint
        checkpoint = torch.load(model_path, map_location=device)
        model_name = checkpoint['model_class']
        optuna_params = checkpoint['optuna_params']
        
        print(f"\nüîç Evaluando {model_name} con Rolling Avanzado")
        # üéØ CONFIGURACI√ìN EXACTA QUE LOGRA 72.41% DA
        if model_name == "GRU_Model":
            # PAR√ÅMETROS EXACTOS del script exitoso
            optuna_params = {
                'hidden_size': 128,
                'learning_rate': 0.0010059426888791,
                'dropout_prob': 0.3441023356173669,
                'batch_size': 16
            }
            seq_length = 60  # EXACTO
            print(f"   üéØ USANDO CONFIGURACI√ìN GANADORA 72.41% DA")
            print(f"   üìä Par√°metros EXACTOS: hidden={optuna_params['hidden_size']}, lr={optuna_params['learning_rate']:.10f}")
        else:
            # Para otros modelos, usar par√°metros del checkpoint
            optuna_params = checkpoint['optuna_params']
            seq_length = checkpoint['seq_length']
            print(f"   üìä Par√°metros: hidden={optuna_params['hidden_size']}, lr={optuna_params['learning_rate']:.6f}")
        
        # Datos completos
        full_data = prepare_full_data()
        
        # Crear modelo para obtener estructura
        input_size = 4  # returns, rsi, sma20, dxy_returns
        
        if model_name == "TLS_LSTMModel":
            model = TLS_LSTMModel(
                input_size=input_size,
                hidden_size=optuna_params['hidden_size'],
                output_size=1,
                dropout_prob=optuna_params['dropout_prob']
            ).to(device)
        elif model_name == "GRU_Model":
            model = GRU_Model(
                input_size=input_size,
                hidden_size=optuna_params['hidden_size'],
                output_size=1,
                dropout_prob=optuna_params['dropout_prob'],
                num_layers=2
            ).to(device)
        elif model_name == "HybridLSTMAttentionModel":
            model = HybridLSTMAttentionModel(
                input_size=input_size,
                hidden_size=optuna_params['hidden_size'],
                output_size=1,
                dropout_prob=optuna_params['dropout_prob']
            ).to(device)
        elif model_name == "BidirectionalDeepLSTMModel":
            model = BidirectionalDeepLSTMModel(
                input_size=input_size,
                hidden_size=optuna_params['hidden_size'],
                output_size=1,
                dropout_prob=optuna_params['dropout_prob']
            ).to(device)
        else:
            print(f"‚ö†Ô∏è Modelo {model_name} no soportado")
            return None
        
        # üöÄ ROLLING FORECAST AVANZADO (con re-entrenamiento)
        advanced_pred, y_real = incremental_rolling_forecast_lstm(
            model, full_data, seq_length, optuna_params, max_predictions=30
        )
        
        if len(advanced_pred) == 0:
            print("‚ùå No se pudieron generar predicciones")
            return None
        
        # üîß VALIDAR Y LIMPIAR DATOS (detectar NaN)
        print(f"   üîç Validando predicciones...")
        print(f"   üìä Predicciones: {len(advanced_pred)}, Reales: {len(y_real)}")
        
        # Verificar NaN en predicciones
        nan_mask_pred = np.isnan(advanced_pred)
        nan_mask_real = np.isnan(y_real)
        
        if np.any(nan_mask_pred):
            print(f"   ‚ö†Ô∏è Encontrados {np.sum(nan_mask_pred)} valores NaN en predicciones")
            print(f"   üîß Limpiando datos...")
        
        if np.any(nan_mask_real):
            print(f"   ‚ö†Ô∏è Encontrados {np.sum(nan_mask_real)} valores NaN en valores reales")
        
        # Crear m√°scara combinada para elementos v√°lidos
        valid_mask = ~(nan_mask_pred | nan_mask_real)
        
        if np.sum(valid_mask) == 0:
            print("‚ùå No hay datos v√°lidos para calcular m√©tricas")
            return None
        
        # Filtrar solo datos v√°lidos
        advanced_pred_clean = advanced_pred[valid_mask]
        y_real_clean = y_real[valid_mask]
        
        print(f"   ‚úÖ Datos v√°lidos: {len(advanced_pred_clean)}/{len(advanced_pred)}")
        
        if len(advanced_pred_clean) < 5:
            print("‚ùå Muy pocos datos v√°lidos para calcular m√©tricas confiables")
            return None
        
        # Verificar valores infinitos
        inf_mask_pred = np.isinf(advanced_pred_clean)
        inf_mask_real = np.isinf(y_real_clean)
        
        if np.any(inf_mask_pred) or np.any(inf_mask_real):
            print(f"   ‚ö†Ô∏è Valores infinitos detectados, limpiando...")
            finite_mask = np.isfinite(advanced_pred_clean) & np.isfinite(y_real_clean)
            advanced_pred_clean = advanced_pred_clean[finite_mask]
            y_real_clean = y_real_clean[finite_mask]
            print(f"   ‚úÖ Datos finitos: {len(advanced_pred_clean)}")
        
        # M√©tricas con datos limpios
        try:
            advanced_rmse = np.sqrt(mean_squared_error(y_real_clean, advanced_pred_clean))
            advanced_r2 = r2_score(y_real_clean, advanced_pred_clean)
        except Exception as e:
            print(f"‚ùå Error calculando m√©tricas: {e}")
            return None
        
        # Directional Accuracy con datos limpios
        def directional_accuracy(y_true, y_pred):
            if len(y_true) <= 1:
                return 0.5
            true_direction = np.diff(y_true) > 0
            pred_direction = np.diff(y_pred) > 0
            return np.mean(true_direction == pred_direction)
        
        advanced_da = directional_accuracy(y_real_clean, advanced_pred_clean)
        
        print(f"   üìä RESULTADOS ROLLING AVANZADO:")
        print(f"   üéØ RMSE: {advanced_rmse:.6f}")
        print(f"   üéØ R¬≤: {advanced_r2:.6f}")
        print(f"   üéØ DA: {advanced_da:.4f} ({advanced_da*100:.1f}%)")
        print(f"   üìä Datos utilizados: {len(advanced_pred_clean)}/{len(advanced_pred)} v√°lidos")
        
        # Comparar con baseline naive (√∫ltimo valor) usando datos limpios
        if len(y_real_clean) > 1:
            naive_pred_clean = y_real_clean[:-1]  # √öltimo valor como predicci√≥n
            naive_da = directional_accuracy(y_real_clean[1:], naive_pred_clean)
        else:
            naive_da = 0.5
        
        da_vs_naive = advanced_da - naive_da
        
        print(f"   üìä vs Naive DA: {naive_da:.4f}")
        print(f"   üéØ Mejora vs Naive: {da_vs_naive:.4f} ({da_vs_naive*100:.1f}%)")
        
        return {
            'model_name': model_name,
            'advanced_rmse': advanced_rmse,
            'advanced_r2': advanced_r2,
            'advanced_da': advanced_da,
            'naive_da': naive_da,
            'da_vs_naive': da_vs_naive,
            'predictions_count': len(advanced_pred),
            'optuna_params': optuna_params
        }
        
    except Exception as e:
        print(f"‚ùå Error evaluando {model_path}: {e}")
        import traceback
        traceback.print_exc()
        return None

def main():
    """Funci√≥n principal para Rolling Forecast Avanzado"""
    print("üöÄ ROLLING FORECAST AVANZADO CON RE-ENTRENAMIENTO")
    print("=" * 70)
    print("üéØ T√©cnica ARIMA: Re-entrenar modelo con cada nuevo dato")
    print("=" * 70)
    
    if torch.cuda.is_available():
        print(f"‚úÖ GPU: {torch.cuda.get_device_name(0)}")
    else:
        print("‚ö†Ô∏è GPU no disponible, se requiere")
        exit(1)

    # Buscar modelos
    current_dir = Path.cwd()
    if current_dir.name == "model":
        models_dir = Path("../modelos") / DEFAULT_PARAMS.TABLENAME
    else:
        models_dir = Path("modelos") / DEFAULT_PARAMS.TABLENAME
    
    optuna_models = list(models_dir.glob("*_optuna_*.pth"))
    
    # Filtrar modelos soportados
    supported_models = []
    for model_path in optuna_models:
        #if any(name in model_path.name for name in ["GRU_Model","TLS_LSTM", "HybridLSTM", "BidirectionalDeep"]):
        if any(name in model_path.name for name in ["BidirectionalDeep"]):
            supported_models.append(model_path)
    
    print(f"\nüîç Modelos soportados: {len(supported_models)}")
    for model_path in supported_models:
        print(f"   üìÅ {model_path.name}")
    
    if not supported_models:
        print("‚ùå No se encontraron modelos soportados")
        return
    
    # Evaluar con rolling avanzado
    results = []
    successful_models = []  # üéØ Lista de modelos exitosos
    
    for i, model_path in enumerate(supported_models, 1):
        print(f"\nüîÑ [{i}/{len(supported_models)}] Rolling Avanzado...")
        
        result = evaluate_model_advanced_rolling(model_path)
        if result:
            results.append(result)
            
            # üîç Agregar a resultados exitosos si DA > 55%
            if result['advanced_da'] > 0.55:
                successful_models.append(result)
    
    if not results:
        print("‚ùå No se pudieron evaluar modelos")
        return
    
    # An√°lisis final
    print(f"\nüèÜ RESUMEN ROLLING AVANZADO")
    print("=" * 80)
    print(f"{'Modelo':<35} {'DA':<8} {'vs Naive':<10} {'Status':<15}")
    print("-" * 80)
    
    improvements = []
    
    for result in results:
        da_improvement = result['da_vs_naive']
        improvements.append(da_improvement)
        
        if result['advanced_da'] > 0.55:  # Umbral exitoso
            status = "üèÜ Excelente"
        elif da_improvement > 0.01:
            status = "‚úÖ Mejor"
        elif da_improvement > 0:
            status = "üü° Leve mejora"
        else:
            status = "‚ùå No mejora"
        
        print(f"{result['model_name']:<35} "
              f"{result['advanced_da']:<8.4f} "
              f"{da_improvement:<10.4f} "
              f"{status:<15}")
    
    # Estad√≠sticas
    avg_improvement = np.mean(improvements)
    best_improvement = max(improvements)
    best_da = max(r['advanced_da'] for r in results)
    
    print(f"\nüìä ESTAD√çSTICAS ROLLING AVANZADO:")
    print(f"   üéØ Mejor DA: {best_da:.4f} ({best_da*100:.1f}%)")
    print(f"   üìà Mejora promedio vs Naive: {avg_improvement:.4f}")
    print(f"   üèÜ Mejor mejora: {best_improvement:.4f}")
    
    # Verificar si superamos umbral de √©xito
    successful_models_count = sum(1 for r in results if r['advanced_da'] > 0.55)
    
    if successful_models_count > 0:
        print(f"\nüéâ ¬°√âXITO! {successful_models_count} modelo(s) superaron 55% DA")
    elif best_da > 0.55:
        print(f"\nüü° Progreso: Mejor DA {best_da*100:.1f}% (cerca del objetivo 55%)")
    else:
        print(f"\n‚ö†Ô∏è A√∫n no se alcanza el objetivo de 55% DA")

    # Guardar resultados
    timestamp = time.strftime('%Y%m%d_%H%M%S')
    current_dir = Path.cwd()
    if current_dir.name == "model":
        results_path = Path("../modelos") / DEFAULT_PARAMS.TABLENAME / f"advanced_rolling_results_{timestamp}.json"
    else:
        results_path = Path("modelos") / DEFAULT_PARAMS.TABLENAME / f"advanced_rolling_results_{timestamp}.json"
    
    results_path.parent.mkdir(exist_ok=True)
    
    summary = {
        'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
        'technique': 'Advanced Rolling Forecast with Re-training',
        'inspiration': 'ARIMA incremental model fitting',
        'best_da': float(best_da),
        'avg_improvement_vs_naive': float(avg_improvement),
        'successful_models': successful_models,
        'total_models': len(results),
        'results': results
    }
    
    with open(results_path, 'w', encoding='utf-8') as f:
        json.dump(summary, f, indent=2, ensure_ascii=False, default=str)
    
    print(f"üíæ Resultados guardados: {results_path}")

if __name__ == "__main__":
    main()
