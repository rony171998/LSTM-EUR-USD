# train_all_models2.py - Entrenar todos los modelos con PAR√ÅMETROS ID√âNTICOS para comparaci√≥n justa
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.preprocessing import RobustScaler
from torch.utils.data import TensorDataset, DataLoader
import time
from datetime import timedelta
from pathlib import Path
import random
from config import DEFAULT_PARAMS
from modelos import (
    TLS_LSTMModel,
    GRU_Model,
    HybridLSTMAttentionModel,
    BidirectionalDeepLSTMModel,
    ContextualLSTMTransformerFlexible,
)

device = torch.device("cuda")

# PAR√ÅMETROS EST√ÅNDAR PARA TODOS LOS MODELOS
STANDARD_PARAMS = {
    'hidden_size': 64,      # Tama√±o est√°ndar balanceado
    'dropout_prob': 0.2,    # Dropout est√°ndar
    'epochs': 120,          # √âpocas suficientes para convergencia
    'batch_size': 32,       # Batch size est√°ndar
    'learning_rate': 0.001, # Learning rate est√°ndar
    'num_layers': 2,        # Capas est√°ndar
    'seq_length': 30        # Longitud de secuencia est√°ndar
}

def set_seed(seed=DEFAULT_PARAMS.SEED):
    """Fijar semillas para reproducibilidad"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    print(f"üé≤ Semilla fijada: {seed}")

def load_multi_asset_data():
    """Cargar EUR/USD + DXY solo (las 2 m√°s importantes)"""
    print("üìä Cargando EUR/USD + DXY...")
        
    # EUR/USD
    eur_file = f"data/{DEFAULT_PARAMS.FILEPATH}"
    eur_df = pd.read_csv(
        eur_file,
        index_col="Fecha",
        parse_dates=True,
        dayfirst=True,
        decimal=",",
        thousands=".",
        converters={
            "√öltimo": lambda x: float(str(x).replace(".", "").replace(",", ".")) if x else np.nan
        }
    )
    eur_df = eur_df.sort_index(ascending=True)
    eur_prices = eur_df["√öltimo"].dropna()
    
    # DXY (si existe)
    dxy_prices = None
    dxy_file = "data/DXY_2010-2024.csv"
    if Path(dxy_file).exists():
        try:
            dxy_df = pd.read_csv(
                dxy_file,
                index_col="Fecha", 
                parse_dates=True,
                dayfirst=True,
                decimal=",",
                thousands=".",
                converters={
                    "√öltimo": lambda x: float(str(x).replace(".", "").replace(",", ".")) if x else np.nan
                }
            )
            dxy_df = dxy_df.sort_index(ascending=True)
            dxy_prices = dxy_df["√öltimo"].dropna()
            print(f"   ‚úÖ DXY cargado: {len(dxy_prices)} registros")
        except:
            print("   ‚ö†Ô∏è DXY no disponible")
    
    print(f"   ‚úÖ EUR/USD: {len(eur_prices)} registros")
    
    return eur_prices, dxy_prices

def create_proven_features(eur_prices, dxy_prices=None):
    """Crear SOLO las caracter√≠sticas que sabemos que funcionan"""
    print("üîß Creando caracter√≠sticas probadas...")
    
    # 1. EUR/USD returns (CR√çTICO)
    eur_returns = eur_prices.pct_change()
    
    # 2. EUR/USD RSI (PROBADO)
    def calculate_rsi(prices, window=14):
        delta = prices.diff()
        gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()
        rs = gain / loss
        return 100 - (100 / (1 + rs))
    
    eur_rsi = calculate_rsi(eur_prices)
    
    # 3. SMA20 (ESTABLE)
    eur_sma20 = eur_prices.rolling(window=20).mean()
    
    # Crear DataFrame base
    features_dict = {
        'price': eur_prices,
        'returns': eur_returns,
        'rsi': eur_rsi,
        'sma20': eur_sma20
    }
    
    # 4. DXY returns (solo si est√° disponible)
    if dxy_prices is not None:
        # Alinear fechas
        common_dates = eur_prices.index.intersection(dxy_prices.index)
        if len(common_dates) > 1000:  # Solo si hay suficientes datos
            dxy_aligned = dxy_prices.reindex(common_dates)
            dxy_returns = dxy_aligned.pct_change()
            
            # Alinear todas las series a fechas comunes
            eur_aligned = eur_prices.reindex(common_dates)
            eur_returns_aligned = eur_aligned.pct_change()
            eur_rsi_aligned = calculate_rsi(eur_aligned)
            eur_sma20_aligned = eur_aligned.rolling(window=20).mean()
            
            features_dict = {
                'price': eur_aligned,
                'returns': eur_returns_aligned,
                'rsi': eur_rsi_aligned,
                'sma20': eur_sma20_aligned,
                'dxy_returns': dxy_returns
            }
            print("   ‚úÖ DXY incluido")
    
    # Crear DataFrame
    features_df = pd.DataFrame(features_dict)
    features_df = features_df.dropna()
    
    print(f"‚úÖ Caracter√≠sticas: {features_df.shape}")
    print(f"   Features: {list(features_df.columns)}")
    
    return features_df

def prepare_data():
    """Preparar datos para entrenamiento"""
    # 1. Cargar datos
    eur_prices, dxy_prices = load_multi_asset_data()
    
    # 2. Crear caracter√≠sticas probadas
    features_df = create_proven_features(eur_prices, dxy_prices)
    
    # 3. Preparar datos
    target_data = features_df['price']
    feature_columns = [col for col in features_df.columns if col != 'price']
    features_data = features_df[feature_columns]
    
    print(f"üìä Features: {len(feature_columns)} | Muestras: {len(features_data)}")
    print(f"   Features: {feature_columns}")
    
    # 4. Split temporal
    train_size = int(len(features_data) * 0.8)
    
    X_train_raw = features_data.iloc[:train_size].values
    X_test_raw = features_data.iloc[train_size:].values
    y_train_raw = target_data.iloc[:train_size].values
    y_test_raw = target_data.iloc[train_size:].values
    
    print(f"üìä Split: {train_size} train, {len(X_test_raw)} test")
    
    # 5. Escalado
    scaler = RobustScaler()
    X_train_scaled = scaler.fit_transform(X_train_raw)
    X_test_scaled = scaler.transform(X_test_raw)
    
    target_scaler = RobustScaler()
    y_train_scaled = target_scaler.fit_transform(y_train_raw.reshape(-1, 1)).flatten()
    y_test_scaled = target_scaler.transform(y_test_raw.reshape(-1, 1)).flatten()
    
    # 6. Crear secuencias
    seq_length = STANDARD_PARAMS['seq_length']  # Usar par√°metro est√°ndar
    
    def create_sequences(X, y, seq_len):
        X_seq, y_seq = [], []
        for i in range(seq_len, len(X)):
            X_seq.append(X[i-seq_len:i])
            y_seq.append(y[i])
        return np.array(X_seq), np.array(y_seq)
    
    X_train_seq, y_train_seq = create_sequences(X_train_scaled, y_train_scaled, seq_length)
    X_test_seq, y_test_seq = create_sequences(X_test_scaled, y_test_scaled, seq_length)
    
    print(f"‚úÖ Secuencias: Train {X_train_seq.shape} | Test {X_test_seq.shape}")
    
    # 7. Tensores
    X_train_tensor = torch.FloatTensor(X_train_seq).to(device)
    y_train_tensor = torch.FloatTensor(y_train_seq).to(device)
    X_test_tensor = torch.FloatTensor(X_test_seq).to(device)
    y_test_tensor = torch.FloatTensor(y_test_seq).to(device)
    
    return {
        'X_train': X_train_tensor,
        'y_train': y_train_tensor,
        'X_test': X_test_tensor,
        'y_test': y_test_tensor,
        'X_train_seq': X_train_seq,
        'X_test_seq': X_test_seq,
        'y_train_seq': y_train_seq,
        'y_test_seq': y_test_seq,
        'target_scaler': target_scaler,
        'feature_columns': feature_columns,
        'seq_length': seq_length
    }

def train_model(model, data, model_name):
    """Entrenar un modelo con PAR√ÅMETROS EST√ÅNDAR"""
    print(f"\nüöÄ Entrenando {model_name} con par√°metros est√°ndar...")
    print("=" * 60)
    print(f"üìã Hidden Size: {STANDARD_PARAMS['hidden_size']}")
    print(f"üìã Dropout: {STANDARD_PARAMS['dropout_prob']}")
    print(f"üìã Epochs: {STANDARD_PARAMS['epochs']}")
    print(f"üìã Batch Size: {STANDARD_PARAMS['batch_size']}")
    print(f"üìã Learning Rate: {STANDARD_PARAMS['learning_rate']}")
    
    start_time = time.time()
    
    # Par√°metros del modelo
    print(f"   Par√°metros: {sum(p.numel() for p in model.parameters()):,}")
    
    # Configuraci√≥n de entrenamiento EST√ÅNDAR
    criterion = nn.MSELoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=STANDARD_PARAMS['learning_rate'])
    
    train_dataset = TensorDataset(data['X_train'], data['y_train'])
    train_loader = DataLoader(train_dataset, batch_size=STANDARD_PARAMS['batch_size'], shuffle=True)
    
    print(f"üî• Entrenando por {STANDARD_PARAMS['epochs']} epochs con par√°metros est√°ndar...")
    
    # Entrenamiento
    model.train()
    for epoch in range(STANDARD_PARAMS['epochs']):
        epoch_loss = 0.0
        batch_count = 0
        
        for batch_X, batch_y in train_loader:
            optimizer.zero_grad()
            
            outputs = model(batch_X).squeeze()
            loss = criterion(outputs, batch_y)
            
            loss.backward()
            optimizer.step()
            
            epoch_loss += loss.item()
            batch_count += 1
        
        if (epoch + 1) % 30 == 0:  # Cada 30 epochs
            avg_loss = epoch_loss / batch_count
            print(f"   Epoch {epoch+1}: Loss = {avg_loss:.6f}")
    
    training_time = time.time() - start_time
    
    # Evaluaci√≥n
    print("üìä Evaluando...")
    model.eval()
    
    with torch.no_grad():
        # Predicciones
        test_pred_scaled = model(data['X_test']).squeeze()
        train_pred_scaled = model(data['X_train']).squeeze()
        
        # Desnormalizar
        test_pred = data['target_scaler'].inverse_transform(test_pred_scaled.cpu().numpy().reshape(-1, 1)).flatten()
        train_pred = data['target_scaler'].inverse_transform(train_pred_scaled.cpu().numpy().reshape(-1, 1)).flatten()
        
        y_test_real = data['target_scaler'].inverse_transform(data['y_test_seq'].reshape(-1, 1)).flatten()
        y_train_real = data['target_scaler'].inverse_transform(data['y_train_seq'].reshape(-1, 1)).flatten()
        
        # M√©tricas
        train_mse = np.mean((train_pred - y_train_real) ** 2)
        test_mse = np.mean((test_pred - y_test_real) ** 2)
        
        train_rmse = np.sqrt(train_mse)
        test_rmse = np.sqrt(test_mse)
        
        train_var = np.var(y_train_real)
        test_var = np.var(y_test_real)
        
        train_r2 = 1 - (train_mse / train_var) if train_var > 0 else 0.0
        test_r2 = 1 - (test_mse / test_var) if test_var > 0 else 0.0
        
        # Directional Accuracy
        def directional_accuracy(y_true, y_pred):
            if len(y_true) <= 1:
                return 0.5
            true_direction = np.diff(y_true) > 0
            pred_direction = np.diff(y_pred) > 0
            return np.mean(true_direction == pred_direction)
        
        train_da = directional_accuracy(y_train_real, train_pred)
        test_da = directional_accuracy(y_test_real, test_pred)
    
    # Guardar modelo
    model_dir = Path("modelos")
    model_dir.mkdir(exist_ok=True)

    model_path = model_dir / DEFAULT_PARAMS.TABLENAME / f"{model_name}_{DEFAULT_PARAMS.FILEPATH}.pth"

    torch.save({
        'model_state_dict': model.state_dict(),
        'model_class': model_name,
        'model_config': get_model_config(model, model_name),
        'feature_columns': data['feature_columns'],
        'seq_length': data['seq_length'],
        'test_rmse': test_rmse,
        'test_r2': test_r2,
        'test_da': test_da,
        'train_rmse': train_rmse,
        'train_r2': train_r2,
        'train_da': train_da,
        'training_time': training_time,
        'epochs': STANDARD_PARAMS['epochs'],
        'standard_params': STANDARD_PARAMS
    }, model_path)
    
    # Resultados
    print(f"\n‚úÖ {model_name} COMPLETADO")
    print("=" * 50)
    print(f"üìä Train RMSE: {train_rmse:.6f}")
    print(f"üìä Test RMSE: {test_rmse:.6f}")
    print(f"üìä Train R¬≤: {train_r2:.6f}")
    print(f"üìä Test R¬≤: {test_r2:.6f}")
    print(f"üìä Train DA: {train_da:.4f} ({train_da*100:.1f}%)")
    print(f"üìä Test DA: {test_da:.4f} ({test_da*100:.1f}%)")
    print(f"‚è±Ô∏è Tiempo: {timedelta(seconds=training_time)}")
    print(f"üíæ Guardado: {model_path}")
    
    return {
        'model_name': model_name,
        'test_rmse': test_rmse,
        'test_r2': test_r2,
        'test_da': test_da,
        'train_rmse': train_rmse,
        'train_r2': train_r2,
        'train_da': train_da,
        'training_time': training_time,
        'model_path': model_path,
        'parameters': sum(p.numel() for p in model.parameters())
    }

def get_model_config(model, model_name):
    """Obtener configuraci√≥n EST√ÅNDAR del modelo"""
    base_config = {
        'hidden_size': STANDARD_PARAMS['hidden_size'],
        'dropout_prob': STANDARD_PARAMS['dropout_prob'],
        'epochs': STANDARD_PARAMS['epochs'],
        'batch_size': STANDARD_PARAMS['batch_size'],
        'learning_rate': STANDARD_PARAMS['learning_rate'],
        'seq_length': STANDARD_PARAMS['seq_length'],
        'config_source': 'Par√°metros est√°ndar para comparaci√≥n justa'
    }
    
    if model_name == "TLS_LSTMModel":
        base_config.update({
            'input_size': model.lstm1.input_size,
            'layers': 2,
            'model_type': 'TLS LSTM b√°sico'
        })
    elif model_name == "TLS_LSTMModel_Optimizado":
        base_config.update({
            'input_size': model.lstm1.input_size,
            'layers': 2,
            'model_type': 'TLS LSTM optimizado (mismos par√°metros)'
        })
    elif model_name == "GRU_Model":
        base_config.update({
            'input_size': model.gru1.input_size,
            'num_layers': model.num_layers,
            'model_type': 'GRU est√°ndar'
        })
    elif model_name == "HybridLSTMAttentionModel":
        base_config.update({
            'input_size': model.lstm1.input_size,
            'layers': 2,
            'attention': True,
            'model_type': 'LSTM + Atenci√≥n'
        })
    elif model_name == "BidirectionalDeepLSTMModel":
        base_config.update({
            'input_size': model.lstm.input_size,
            'bidirectional': True,
            'num_layers': 2,
            'model_type': 'LSTM Bidireccional'
        })
    elif model_name == "ContextualLSTMTransformerFlexible":
        base_config.update({
            'seq_len': model.seq_len,
            'feature_dim': model.feature_dim,
            'window_size': model.window_size,
            'lstm_units': STANDARD_PARAMS['hidden_size'],  # Usar est√°ndar
            'num_heads': 2,
            'embed_dim': STANDARD_PARAMS['hidden_size'],   # Usar est√°ndar
            'dropout_rate': STANDARD_PARAMS['dropout_prob'], # Usar est√°ndar
            'model_type': 'LSTM + Transformer'
        })
    
    return base_config

def main():
    """Funci√≥n principal para entrenar todos los modelos con PAR√ÅMETROS ID√âNTICOS"""
    print("üéØ ENTRENAMIENTO CON PAR√ÅMETROS EST√ÅNDAR ID√âNTICOS")
    print("=" * 70)
    print("üìã PAR√ÅMETROS EST√ÅNDAR PARA TODOS LOS MODELOS:")
    print(f"   Hidden Size: {STANDARD_PARAMS['hidden_size']}")
    print(f"   Dropout: {STANDARD_PARAMS['dropout_prob']}")
    print(f"   Epochs: {STANDARD_PARAMS['epochs']}")
    print(f"   Batch Size: {STANDARD_PARAMS['batch_size']}")
    print(f"   Learning Rate: {STANDARD_PARAMS['learning_rate']}")
    print(f"   Sequence Length: {STANDARD_PARAMS['seq_length']}")
    print("=" * 70)
    print("üé≤ Usando semillas fijas para reproducibilidad completa")
    print("=" * 70)
    
    # Fijar semilla para reproducibilidad
    set_seed(42)
    
    if torch.cuda.is_available():
        print(f"‚úÖ GPU: {torch.cuda.get_device_name(0)}")
    else:
        print("‚ö†Ô∏è Usando CPU")
    
    # Preparar datos una vez
    data = prepare_data()
    input_size = data['X_train'].shape[2]
    
    # Lista de modelos con PAR√ÅMETROS ID√âNTICOS
    models_to_train = []
    
    # 1. TLS_LSTMModel - Con par√°metros est√°ndar
    print("\nüîß TLS_LSTMModel - Par√°metros est√°ndar")
    models_to_train.append({
        'model': TLS_LSTMModel(
            input_size=input_size, 
            hidden_size=STANDARD_PARAMS['hidden_size'],
            output_size=1, 
            dropout_prob=STANDARD_PARAMS['dropout_prob']
        ).to(device),
        'name': 'TLS_LSTMModel'
    })
    
    # 2. GRU_Model - Con par√°metros est√°ndar
    print("üîß GRU_Model - Par√°metros est√°ndar")
    models_to_train.append({
        'model': GRU_Model(
            input_size=input_size, 
            hidden_size=STANDARD_PARAMS['hidden_size'],
            output_size=1, 
            dropout_prob=STANDARD_PARAMS['dropout_prob'],
            num_layers=STANDARD_PARAMS['num_layers']
        ).to(device),
        'name': 'GRU_Model'
    })
    
    # 3. HybridLSTMAttentionModel - Con par√°metros est√°ndar
    print("üîß HybridLSTMAttentionModel - Par√°metros est√°ndar")
    models_to_train.append({
        'model': HybridLSTMAttentionModel(
            input_size=input_size, 
            hidden_size=STANDARD_PARAMS['hidden_size'],
            output_size=1, 
            dropout_prob=STANDARD_PARAMS['dropout_prob']
        ).to(device),
        'name': 'HybridLSTMAttentionModel'
    })
    
    # 4. BidirectionalDeepLSTMModel - Con par√°metros est√°ndar
    print("üîß BidirectionalDeepLSTMModel - Par√°metros est√°ndar")
    models_to_train.append({
        'model': BidirectionalDeepLSTMModel(
            input_size=input_size, 
            hidden_size=STANDARD_PARAMS['hidden_size'],
            output_size=1, 
            dropout_prob=STANDARD_PARAMS['dropout_prob']
        ).to(device),
        'name': 'BidirectionalDeepLSTMModel'
    })
    
    # 5. TLS_LSTMModel "Optimizado" - CON LOS MISMOS PAR√ÅMETROS EST√ÅNDAR
    print("üîß TLS_LSTMModel_Optimizado - Mismos par√°metros est√°ndar")
    models_to_train.append({
        'model': TLS_LSTMModel(
            input_size=input_size, 
            hidden_size=STANDARD_PARAMS['hidden_size'],  # MISMO que b√°sico
            output_size=1, 
            dropout_prob=STANDARD_PARAMS['dropout_prob']  # MISMO que b√°sico
        ).to(device),
        'name': 'TLS_LSTMModel_Optimizado'
    })
    
    # 6. ContextualLSTMTransformerFlexible - Con par√°metros est√°ndar adaptados
    print("üîß ContextualLSTMTransformerFlexible - Par√°metros est√°ndar adaptados")
    models_to_train.append({
        'model': ContextualLSTMTransformerFlexible(
            seq_len=STANDARD_PARAMS['seq_length'],
            feature_dim=input_size, 
            output_size=1,
            window_size=6,
            max_neighbors=1,
            lstm_units=STANDARD_PARAMS['hidden_size'],    # Usar est√°ndar
            num_heads=2,
            embed_dim=STANDARD_PARAMS['hidden_size'],     # Usar est√°ndar  
            dropout_rate=STANDARD_PARAMS['dropout_prob']  # Usar est√°ndar
        ).to(device),
        'name': 'ContextualLSTMTransformerFlexible'
    })
    
    # Entrenar todos los modelos con par√°metros ID√âNTICOS
    results = []
    total_start_time = time.time()
    
    for i, model_info in enumerate(models_to_train, 1):
        print(f"\nüîÑ [{i}/{len(models_to_train)}] Entrenando {model_info['name']}...")
        print(f"   üìã MISMOS par√°metros est√°ndar para comparaci√≥n justa")
        
        # Re-fijar semilla antes de cada modelo para mayor reproducibilidad
        set_seed(42 + i)
        
        try:
            result = train_model(
                model_info['model'], 
                data, 
                model_info['name']
            )
            results.append(result)
            
        except Exception as e:
            print(f"‚ùå Error entrenando {model_info['name']}: {e}")
            import traceback
            traceback.print_exc()
    
    total_time = time.time() - total_start_time
    
    # Resumen final
    print("\nüèÜ RESUMEN FINAL - COMPARACI√ìN CON PAR√ÅMETROS ID√âNTICOS")
    print("=" * 85)
    print("üìã TODOS los modelos entrenados con PAR√ÅMETROS EST√ÅNDAR ID√âNTICOS")
    print(f"   Hidden Size: {STANDARD_PARAMS['hidden_size']} | Dropout: {STANDARD_PARAMS['dropout_prob']} | Epochs: {STANDARD_PARAMS['epochs']}")
    print("üé≤ Semillas fijas para reproducibilidad completa")
    print("-" * 85)
    
    # Ordenar por RMSE
    results_sorted = sorted(results, key=lambda x: x['test_rmse'])
    
    print(f"{'Rank':<4} {'Modelo':<35} {'RMSE':<8} {'R¬≤':<8} {'DA':<6} {'Params':<8} {'Tiempo':<12} {'Config'}")
    print("-" * 85)
    
    for i, result in enumerate(results_sorted, 1):
        emoji = "ü•á" if i == 1 else "ü•à" if i == 2 else "ü•â" if i == 3 else "  "
        config_info = "Est√°ndar‚úì"
        print(f"{emoji}{i:<3} {result['model_name']:<35} {result['test_rmse']:<8.6f} {result['test_r2']:<8.4f} {result['test_da']:<6.3f} {result['parameters']:<8,} {str(timedelta(seconds=result['training_time'])):<12} {config_info}")
    
    print(f"\n‚è±Ô∏è Tiempo total: {timedelta(seconds=total_time)}")
    print(f"üíæ Todos los modelos guardados en: modelos/")
    print(f"üìä Comparaci√≥n justa: TODOS con par√°metros id√©nticos")
    
    # Guardar resumen
    summary_path = Path("modelos") / DEFAULT_PARAMS.TABLENAME / "training_summary_standard_params.txt"
    summary_path.parent.mkdir(exist_ok=True)
    with open(summary_path, 'w', encoding='utf-8') as f:
        f.write("RESUMEN DE ENTRENAMIENTO CON PAR√ÅMETROS EST√ÅNDAR ID√âNTICOS\n")
        f.write("=" * 60 + "\n")
        f.write(f"Fecha: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\n")
        f.write(f"Semilla fija: 42 (reproducibilidad garantizada)\n")
        f.write(f"Features: {data['feature_columns']}\n")
        f.write(f"Secuencias: {data['seq_length']}\n")
        f.write("PAR√ÅMETROS EST√ÅNDAR UTILIZADOS PARA TODOS LOS MODELOS:\n")
        f.write(f"  Hidden Size: {STANDARD_PARAMS['hidden_size']}\n")
        f.write(f"  Dropout: {STANDARD_PARAMS['dropout_prob']}\n")
        f.write(f"  Epochs: {STANDARD_PARAMS['epochs']}\n")
        f.write(f"  Batch Size: {STANDARD_PARAMS['batch_size']}\n")
        f.write(f"  Learning Rate: {STANDARD_PARAMS['learning_rate']}\n")
        f.write(f"  Sequence Length: {STANDARD_PARAMS['seq_length']}\n\n")
        
        f.write("RESULTADOS (Comparaci√≥n justa con par√°metros id√©nticos):\n")
        f.write("-" * 40 + "\n")
        for i, result in enumerate(results_sorted, 1):
            f.write(f"{i}. {result['model_name']}\n")
            f.write(f"   RMSE: {result['test_rmse']:.6f}\n")
            f.write(f"   R¬≤: {result['test_r2']:.6f}\n")
            f.write(f"   DA: {result['test_da']:.4f}\n")
            f.write(f"   Par√°metros: {result['parameters']:,}\n")
            f.write(f"   Tiempo: {timedelta(seconds=result['training_time'])}\n")
            f.write(f"   Archivo: {result['model_path']}\n\n")
        
        f.write("VENTAJAS DE ESTA COMPARACI√ìN:\n")
        f.write("-" * 40 + "\n")
        f.write("‚Ä¢ Comparaci√≥n justa: TODOS con par√°metros id√©nticos\n")
        f.write("‚Ä¢ Elimina sesgo por configuraci√≥n diferente\n")
        f.write("‚Ä¢ Eval√∫a arquitectura pura, no hiperpar√°metros\n")
        f.write("‚Ä¢ Reproducibilidad completa con semillas fijas\n")
        f.write("‚Ä¢ Misma capacidad de aprendizaje para todos\n")
    
    print(f"üìÑ Resumen detallado guardado en: {summary_path}")

if __name__ == "__main__":
    main()
