"""
Script para verificar la disponibilidad y capacidades de GPU con PyTorch
Autor: Proyecto LSTM-EUR-USD
Fecha: 2025-02-08
"""

import torch
import platform
import sys
import subprocess
import time
import numpy as np

def check_system_info():
    """Informaci√≥n b√°sica del sistema"""
    print("=" * 60)
    print("üñ•Ô∏è  INFORMACI√ìN DEL SISTEMA")
    print("=" * 60)
    print(f"Sistema Operativo: {platform.system()} {platform.release()}")
    print(f"Arquitectura: {platform.machine()}")
    print(f"Procesador: {platform.processor()}")
    print(f"Python: {sys.version}")
    print(f"PyTorch: {torch.__version__}")
    print()

def check_cuda_availability():
    """Verificar disponibilidad de CUDA"""
    print("=" * 60)
    print("üöÄ VERIFICACI√ìN DE CUDA")
    print("=" * 60)
    
    # Verificar si CUDA est√° disponible
    cuda_available = torch.cuda.is_available()
    print(f"CUDA disponible: {'‚úÖ S√ç' if cuda_available else '‚ùå NO'}")
    
    if cuda_available:
        print(f"Versi√≥n de CUDA: {torch.version.cuda}")
        print(f"cuDNN disponible: {'‚úÖ S√ç' if torch.backends.cudnn.enabled else '‚ùå NO'}")
        if torch.backends.cudnn.enabled:
            print(f"Versi√≥n de cuDNN: {torch.backends.cudnn.version()}")
        
        # N√∫mero de dispositivos GPU
        num_gpus = torch.cuda.device_count()
        print(f"N√∫mero de GPUs: {num_gpus}")
        
        # Informaci√≥n de cada GPU
        for i in range(num_gpus):
            gpu_name = torch.cuda.get_device_name(i)
            gpu_memory = torch.cuda.get_device_properties(i).total_memory / 1024**3
            print(f"GPU {i}: {gpu_name} ({gpu_memory:.1f} GB)")
        
        # GPU actual
        current_device = torch.cuda.current_device()
        print(f"GPU actual: {current_device}")
        print(f"Nombre GPU actual: {torch.cuda.get_device_name(current_device)}")
        
    else:
        print("üí° Posibles soluciones:")
        print("   1. Instalar PyTorch con soporte CUDA")
        print("   2. Verificar drivers de NVIDIA")
        print("   3. Verificar compatibilidad GPU-CUDA")
    
    print()

def check_device_properties():
    """Propiedades detalladas de los dispositivos"""
    if not torch.cuda.is_available():
        return
    
    print("=" * 60)
    print("üîß PROPIEDADES DETALLADAS DE GPU")
    print("=" * 60)
    
    for i in range(torch.cuda.device_count()):
        props = torch.cuda.get_device_properties(i)
        print(f"\nüì± GPU {i}: {props.name}")
        print("-" * 40)
        print(f"Memoria total: {props.total_memory / 1024**3:.2f} GB")
        print(f"Multiprocesadores: {props.multi_processor_count}")
        print(f"Memoria compartida por bloque: {props.shared_memory_per_block / 1024:.1f} KB")
        print(f"Memoria compartida por multiprocesador: {props.shared_memory_per_multiprocessor / 1024:.1f} KB")
        print(f"Registros por bloque: {props.regs_per_block:,}")
        print(f"Registros por multiprocesador: {props.regs_per_multiprocessor:,}")
        print(f"M√°ximo hilos por bloque: {props.max_threads_per_block:,}")
        print(f"M√°ximo hilos por multiprocesador: {props.max_threads_per_multi_processor:,}")
        print(f"Capacidad computacional: {props.major}.{props.minor}")
        print(f"Frecuencia de reloj: {props.clock_rate / 1000:.0f} MHz")
        print(f"Frecuencia de memoria: {props.memory_clock_rate / 1000:.0f} MHz")
        print(f"Ancho de bus de memoria: {props.memory_bus_width} bits")

def check_memory_usage():
    """Verificar uso de memoria de GPU"""
    if not torch.cuda.is_available():
        return
    
    print("\n" + "=" * 60)
    print("üíæ USO DE MEMORIA GPU")
    print("=" * 60)
    
    for i in range(torch.cuda.device_count()):
        torch.cuda.set_device(i)
        
        # Limpiar cache
        torch.cuda.empty_cache()
        
        # Obtener informaci√≥n de memoria
        total_memory = torch.cuda.get_device_properties(i).total_memory
        allocated_memory = torch.cuda.memory_allocated(i)
        cached_memory = torch.cuda.memory_reserved(i)
        free_memory = total_memory - cached_memory
        
        print(f"\nüì± GPU {i}:")
        print(f"  Memoria total: {total_memory / 1024**3:.2f} GB")
        print(f"  Memoria asignada: {allocated_memory / 1024**3:.3f} GB")
        print(f"  Memoria en cach√©: {cached_memory / 1024**3:.3f} GB")
        print(f"  Memoria libre: {free_memory / 1024**3:.2f} GB")
        print(f"  Utilizaci√≥n: {(cached_memory / total_memory) * 100:.1f}%")

def performance_test():
    """Test de rendimiento b√°sico GPU vs CPU"""
    print("\n" + "=" * 60)
    print("‚ö° TEST DE RENDIMIENTO")
    print("=" * 60)
    
    # Crear datos de prueba
    size = 5000
    device_cpu = torch.device('cpu')
    
    print(f"Creando matrices {size}x{size} para prueba...")
    
    # Test CPU
    print("\nüñ•Ô∏è  Test CPU:")
    a_cpu = torch.randn(size, size, device=device_cpu)
    b_cpu = torch.randn(size, size, device=device_cpu)
    
    start_time = time.time()
    c_cpu = torch.matmul(a_cpu, b_cpu)
    cpu_time = time.time() - start_time
    print(f"  Multiplicaci√≥n de matrices: {cpu_time:.3f} segundos")
    
    # Test GPU (si disponible)
    if torch.cuda.is_available():
        device_gpu = torch.device('cuda:0')
        
        print("\nüöÄ Test GPU:")
        
        # Transferir datos a GPU
        transfer_start = time.time()
        a_gpu = a_cpu.to(device_gpu)
        b_gpu = b_cpu.to(device_gpu)
        transfer_time = time.time() - transfer_start
        print(f"  Transferencia CPU‚ÜíGPU: {transfer_time:.3f} segundos")
        
        # Operaci√≥n en GPU
        start_time = time.time()
        c_gpu = torch.matmul(a_gpu, b_gpu)
        torch.cuda.synchronize()  # Esperar a que termine
        gpu_time = time.time() - start_time
        print(f"  Multiplicaci√≥n de matrices: {gpu_time:.3f} segundos")
        
        # Transferir resultado de vuelta
        transfer_start = time.time()
        c_gpu_cpu = c_gpu.to(device_cpu)
        transfer_back_time = time.time() - transfer_start
        print(f"  Transferencia GPU‚ÜíCPU: {transfer_back_time:.3f} segundos")
        
        # Comparar resultados
        difference = torch.norm(c_cpu - c_gpu_cpu).item()
        print(f"  Diferencia entre resultados: {difference:.2e}")
        
        # Speedup
        total_gpu_time = transfer_time + gpu_time + transfer_back_time
        speedup_compute = cpu_time / gpu_time
        speedup_total = cpu_time / total_gpu_time
        
        print(f"\nüìä An√°lisis de rendimiento:")
        print(f"  Speedup solo computaci√≥n: {speedup_compute:.2f}x")
        print(f"  Speedup total (con transferencias): {speedup_total:.2f}x")
        
        if speedup_compute > 1:
            print(f"  ‚úÖ GPU es {speedup_compute:.1f}x m√°s r√°pida para computaci√≥n")
        else:
            print(f"  ‚ùå CPU es m√°s r√°pida para este tama√±o de problema")
            
        if speedup_total > 1:
            print(f"  ‚úÖ Beneficio neto: {speedup_total:.1f}x incluyendo transferencias")
        else:
            print(f"  ‚ö†Ô∏è  Overhead de transferencias reduce beneficio")

def lstm_gpu_test():
    """Test espec√≠fico con LSTM para deep learning"""
    if not torch.cuda.is_available():
        return
    
    print("\n" + "=" * 60)
    print("üß† TEST LSTM EN GPU")
    print("=" * 60)
    
    # Par√°metros del test
    batch_size = 32
    seq_length = 120
    input_size = 3
    hidden_size = 512
    
    print(f"Configuraci√≥n del test:")
    print(f"  Batch size: {batch_size}")
    print(f"  Longitud secuencia: {seq_length}")
    print(f"  Dimensi√≥n entrada: {input_size}")
    print(f"  Dimensi√≥n oculta: {hidden_size}")
    
    # Crear modelo y datos
    device_cpu = torch.device('cpu')
    device_gpu = torch.device('cuda:0')
    
    # Test CPU
    print(f"\nüñ•Ô∏è  Test LSTM en CPU:")
    lstm_cpu = torch.nn.LSTM(input_size, hidden_size, batch_first=True).to(device_cpu)
    data_cpu = torch.randn(batch_size, seq_length, input_size, device=device_cpu)
    
    start_time = time.time()
    output_cpu, _ = lstm_cpu(data_cpu)
    cpu_time = time.time() - start_time
    print(f"  Forward pass: {cpu_time:.3f} segundos")
    
    # Test GPU
    print(f"\nüöÄ Test LSTM en GPU:")
    lstm_gpu = torch.nn.LSTM(input_size, hidden_size, batch_first=True).to(device_gpu)
    data_gpu = data_cpu.to(device_gpu)
    
    start_time = time.time()
    output_gpu, _ = lstm_gpu(data_gpu)
    torch.cuda.synchronize()
    gpu_time = time.time() - start_time
    print(f"  Forward pass: {gpu_time:.3f} segundos")
    
    # Comparar
    speedup = cpu_time / gpu_time
    print(f"\nüìä Speedup LSTM: {speedup:.2f}x")
    
    if speedup > 1:
        print(f"  ‚úÖ GPU es {speedup:.1f}x m√°s r√°pida para LSTM")
        print(f"  üí° Recomendado usar GPU para entrenamiento")
    else:
        print(f"  ‚ö†Ô∏è  CPU es m√°s r√°pida para este tama√±o de LSTM")
        print(f"  üí° Considera usar CPU para esta configuraci√≥n")

def check_nvidia_smi():
    """Verificar nvidia-smi si est√° disponible"""
    print("\n" + "=" * 60)
    print("üîß NVIDIA SYSTEM MANAGEMENT")
    print("=" * 60)
    
    try:
        result = subprocess.run(['nvidia-smi'], capture_output=True, text=True, timeout=10)
        if result.returncode == 0:
            print("‚úÖ nvidia-smi disponible:")
            print(result.stdout)
        else:
            print("‚ùå Error ejecutando nvidia-smi:")
            print(result.stderr)
    except subprocess.TimeoutExpired:
        print("‚è±Ô∏è  Timeout ejecutando nvidia-smi")
    except FileNotFoundError:
        print("‚ùå nvidia-smi no encontrado")
        print("üí° Instala drivers de NVIDIA para m√°s informaci√≥n")
    except Exception as e:
        print(f"‚ùå Error: {e}")

def recommendations():
    """Recomendaciones basadas en la configuraci√≥n"""
    print("\n" + "=" * 60)
    print("üí° RECOMENDACIONES")
    print("=" * 60)
    
    if torch.cuda.is_available():
        num_gpus = torch.cuda.device_count()
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
        
        print("‚úÖ Configuraci√≥n GPU detectada:")
        print(f"  ‚Ä¢ {num_gpus} GPU(s) disponible(s)")
        print(f"  ‚Ä¢ {gpu_memory:.1f} GB de memoria GPU")
        
        print("\nüöÄ Para optimizar entrenamiento:")
        if gpu_memory >= 8:
            print("  ‚Ä¢ Usa batch_size = 32-64 para LSTM")
            print("  ‚Ä¢ Habilita mixed precision (fp16) para mayor velocidad")
            print("  ‚Ä¢ Considera modelos m√°s grandes (hidden_size = 512-1024)")
        elif gpu_memory >= 4:
            print("  ‚Ä¢ Usa batch_size = 16-32 para LSTM")
            print("  ‚Ä¢ Considera mixed precision (fp16)")
            print("  ‚Ä¢ Mant√©n hidden_size = 256-512")
        else:
            print("  ‚Ä¢ Usa batch_size = 8-16 para LSTM")
            print("  ‚Ä¢ Considera entrenar en CPU para modelos peque√±os")
            print("  ‚Ä¢ Limita hidden_size = 128-256")
        
        print("\nüìã Configuraci√≥n recomendada para config.py:")
        print("  BATCH_SIZE = 32 if gpu_memory >= 8 else 16")
        print("  HIDDEN_SIZE = 512 if gpu_memory >= 8 else 256")
        print("  device = torch.device('cuda')")
        
    else:
        print("‚ùå GPU no disponible:")
        print("\nüîß Para habilitar GPU:")
        print("  1. Instala drivers NVIDIA actualizados")
        print("  2. Reinstala PyTorch con CUDA:")
        print("     pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118")
        print("  3. Verifica compatibilidad GPU-CUDA")
        
        print("\n‚öôÔ∏è  Configuraci√≥n para CPU:")
        print("  ‚Ä¢ Usa batch_size = 8-16")
        print("  ‚Ä¢ Limita hidden_size = 128-256")
        print("  ‚Ä¢ Aumenta num_workers para DataLoader")
        print("  ‚Ä¢ Considera usar modelos m√°s simples")

def main():
    """Funci√≥n principal"""
    print("üîç VERIFICACI√ìN COMPLETA DE GPU PARA PYTORCH")
    print("=" * 60)
    print("Este script verifica la disponibilidad y rendimiento de GPU")
    print("para entrenamiento de modelos de deep learning con PyTorch.")
    print()
    
    # Ejecutar todos los tests
    check_system_info()
    check_cuda_availability()
    check_device_properties()
    check_memory_usage()
    performance_test()
    lstm_gpu_test()
    check_nvidia_smi()
    recommendations()
    
    print("\n" + "=" * 60)
    print("‚úÖ VERIFICACI√ìN COMPLETADA")
    print("=" * 60)
    print("Usa esta informaci√≥n para optimizar la configuraci√≥n")
    print("de entrenamiento en tu proyecto LSTM-EUR-USD.")

if __name__ == "__main__":
    main()