# üìä RESUMEN COMPLETO DE MODELOS ENTRENADOS - LSTM EUR/USD

## üéØ Informaci√≥n General del Dataset
- **Dataset**: EUR/USD + DXY (2010-2024)
- **Registros totales**: 3,882 muestras
- **Features utilizadas**: 4 ['returns', 'rsi', 'sma20', 'dxy_returns']
- **Split temporal**: 80% train (3,105) / 20% test (777)
- **Secuencias**: Train (3,075, 30, 4) | Test (747, 30, 4)
- **Escalado**: RobustScaler (sin quantile_range personalizado)
- **Criterio de p√©rdida**: MSE Loss
- **Optimizador**: Adam (lr=0.001)
- **Batch size**: 32
- **GPU**: NVIDIA GeForce RTX 4060

---

## üìã TABLA COMPARATIVA COMPLETA

| **Modelo** | **Capas** | **Hidden Size** | **Output Size** | **Seq Length** | **Par√°metros** | **√âpocas** | **Dropout** | **Caracter√≠sticas Especiales** |
|------------|-----------|-----------------|-----------------|----------------|----------------|------------|-------------|--------------------------------|
| **TLS_LSTMModel** | 2 LSTM | 50 | 1 | 30 | 31,651 | 100 | 0.2 | 2 capas LSTM secuenciales con dropout |
| **GRU_Model** | 2 GRU | 50 | 1 | 30 | 54,351 | 100 | 0.2 | 2 capas GRU con num_layers=2 |
| **HybridLSTMAttentionModel** | 2 LSTM + Attention | 50 | 1 | 30 | 34,202 | 100 | 0.2 | LSTM + Mecanismo de Atenci√≥n + FC |
| **BidirectionalDeepLSTMModel** | 2 LSTM Bidireccional | 50 | 1 | 30 | 88,301 | 100 | 0.2 | LSTM Bidireccional + Capas Profundas |
| **TLS_LSTMModel Optimizado** | 2 LSTM | 128 | 1 | 30 | 200,833 | 150 | 0.1 | Versi√≥n optimizada con mayor capacidad |
| **ContextualLSTMTransformerFlexible** | LSTM + Transformer | 32 (LSTM) | 1 | 30 | 134,977 | 150 | 0.2 | LSTM + Self-Attention + Cross-Attention |

---

## üèÜ RESULTADOS DE RENDIMIENTO

| **Modelo** | **Test RMSE** | **Test R¬≤** | **Train DA** | **Test DA** | **Tiempo** | **Ranking** |
|------------|---------------|-------------|--------------|-------------|------------|-------------|
| **HybridLSTMAttentionModel** | **0.006669** | **0.958940** | 49.1% | 50.4% | 1:36 | ü•á **1¬∞** |
| **TLS_LSTMModel Optimizado** | **0.006269** | **0.963723** | 49.5% | 50.3% | 1:44 | ü•à **2¬∞** |
| **BidirectionalDeepLSTMModel** | 0.012893 | 0.846564 | 48.9% | **51.7%** | 1:20 | ü•â **3¬∞** |
| **GRU_Model** | 0.014184 | 0.814289 | 48.6% | 49.6% | 1:19 | **4¬∞** |
| **TLS_LSTMModel** | 0.014611 | 0.802931 | 48.6% | 50.7% | 1:12 | **5¬∞** |
| **ContextualLSTMTransformerFlexible** | 0.021419 | 0.576492 | 51.2% | **51.3%** | 10:03 | **6¬∞** |

---

## üîß DETALLES ARQUITECT√ìNICOS ESPEC√çFICOS

### 1. **TLS_LSTMModel (B√°sico)**
```python
- LSTM1: input_size=4 ‚Üí hidden_size=50
- Dropout1: 0.2
- LSTM2: hidden_size=50 ‚Üí hidden_size=50  
- Dropout2: 0.2
- FC: hidden_size=50 ‚Üí output_size=1
```

### 2. **GRU_Model**
```python
- GRU1: input_size=4 ‚Üí hidden_size=50, num_layers=2
- Dropout1: 0.2
- GRU2: hidden_size=50 ‚Üí hidden_size=50, num_layers=2
- Dropout2: 0.2
- FC: hidden_size=50 ‚Üí output_size=1
```

### 3. **HybridLSTMAttentionModel** ‚≠ê
```python
- LSTM1: input_size=4 ‚Üí hidden_size=50
- Dropout1: 0.2
- LSTM2: hidden_size=50 ‚Üí hidden_size=50
- Attention: Linear(50‚Üí25) + Tanh + Linear(25‚Üí1) + Softmax
- FC: Linear(50‚Üí25) + ReLU + Dropout(0.2) + Linear(25‚Üí1)
```

### 4. **BidirectionalDeepLSTMModel**
```python
- LSTM: input_size=4 ‚Üí hidden_size=50, bidirectional=True, num_layers=2
- Dropout: 0.2
- FC: Linear(100‚Üí50) + ReLU + Dropout(0.2) + Linear(50‚Üí1)
```

### 5. **TLS_LSTMModel Optimizado** ‚≠ê
```python
- LSTM1: input_size=4 ‚Üí hidden_size=128
- Dropout1: 0.1 (reducido)
- LSTM2: hidden_size=128 ‚Üí hidden_size=128
- Dropout2: 0.1 (reducido)
- FC: hidden_size=128 ‚Üí output_size=1
- √âpocas: 150 (aumentadas)
```

### 6. **ContextualLSTMTransformerFlexible**
```python
- Window Size: 6 (divide seq_len=30 en 5 ventanas)
- Max Neighbors: 1
- LSTM Units: 32 (bidireccional ‚Üí 64)
- Num Heads: 2
- Embed Dim: 64
- Componentes:
  * ReshapeToWindows
  * LSTMWithSelfAttention (LSTM + MultiheadAttention)
  * CrossAttentionBlock (para ventanas contextuales)
  * Final Dense: 64 ‚Üí 1
```

---

## üìà AN√ÅLISIS COMPARATIVO

### üèÖ **CAMPEONES POR CATEGOR√çA:**

1. **üéØ Mejor RMSE**: TLS_LSTMModel Optimizado (0.006269)
2. **üìä Mejor R¬≤**: TLS_LSTMModel Optimizado (0.963723)
3. **üé≤ Mejor Directional Accuracy**: ContextualLSTMTransformerFlexible (51.3%)
4. **‚ö° M√°s Eficiente**: HybridLSTMAttentionModel (34K par√°metros, 2¬∞ lugar)
5. **üöÄ M√°s R√°pido**: TLS_LSTMModel (1:12 min)

### üìä **PATRONES IDENTIFICADOS:**

1. **Tama√±o vs Rendimiento**: M√°s par√°metros no siempre = mejor rendimiento
2. **Atenci√≥n Funciona**: Los 2 mejores modelos usan atenci√≥n o mayor capacidad
3. **Dropout Optimizado**: Reducir dropout (0.1 vs 0.2) mejor√≥ el rendimiento
4. **√âpocas Importan**: 150 √©pocas vs 100 mejor√≥ la convergencia
5. **Transformer Complejo**: Alta capacidad direccional pero overfitting en RMSE

### ‚ö†Ô∏è **OBSERVACIONES T√âCNICAS:**

- **Overfitting**: ContextualLSTMTransformer muestra Train R¬≤ (0.996) vs Test R¬≤ (0.576)
- **Sweet Spot**: 34K-200K par√°metros parecen √≥ptimos para este dataset
- **Directional vs RMSE**: Existe trade-off entre precisi√≥n num√©rica y direccional
- **GPU Necesaria**: Todos los modelos requieren CUDA para entrenamiento eficiente

---

## üéØ CONCLUSIONES FINALES

1. **Top Tier**: HybridLSTMAttentionModel y TLS_LSTM Optimizado est√°n en una liga propia
2. **Atenci√≥n > Complejidad**: Mecanismos de atenci√≥n superan a arquitecturas m√°s complejas
3. **Configuraci√≥n Importa**: Ajustar dropout, √©pocas y hidden_size es crucial
4. **Especializaci√≥n**: Algunos modelos mejor para RMSE, otros para direccionalidad

---

*Generado el 13 de agosto de 2025 - Experimento LSTM EUR/USD*
